---
title: "Semesterarbeit_4_Cristian_Ruiz_1"
output:
  html_document: default
  pdf_document: default
date: "2024-01-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#Teil 1

## Einleitung

Bei der Planung vom Sale (Ausverkauf) bei der Jelmoli AG wird oft diskutiert, ob ein grÃ¶sseres Angebot an stark rabattierten Artikeln, insbesondere zu Beginn des Sales, zu einer Umsatzsteigerung fÃ¼hren kÃ¶nnte. Es wird angenommen, dass Artikel mit hohen Rabatten eine bessere Verkaufsleistung zeigen, obwohl dies nicht unbedingt einen hÃ¶heren Umsatz bedeutet, da der Rabatt den Ertrag beeinflusst. Diese Analyse zielt nicht darauf ab, die finanzielle TragfÃ¤higkeit eines solchen Angebots zu bewerten, sondern vielmehr zu ermitteln, ob nichtrabattierte Artikel wirklich eine deutlich schlechtere VerkaufshÃ¤ufigkeit aufweisen als hochrabttierte Artikel. DafÃ¼r werden Daten vom ersten Verkaufstag des Sales, dem 27. Dezember, in der Jelmoli-Filiale an der ZÃ¼rcher Bahnhofstrasse verwendet.

Die Methodik umfasst:

**1. Kategorisierung und Visualisierung der Rabatte:** Artikel werden basierend auf ihrem Rabattsatz kategorisiert und in einem Histogramm visualisiert, um die Verteilung zu analysieren.

**2. Auswahl der Verteilung:** Bestimmung einer passenden theoretischen Verteilung fÃ¼r nichtrabttierte Artikel und Vergleich dieser mit der tatsÃ¤chlichen Verteilung.

**3. Goodness-of-Fit-Test:** DurchfÃ¼hrung eines ğœ’Â²-Goodness-of-Fit-Tests, um die Eignung der gewÃ¤hlten Verteilung zu beurteilen und die Hypothese zu testen, dass nichtrabattierte Artikel deutlich schlechtere Verkaufsleistung zeigen.


## Laden der Pakete

```{r}
# Pakete laden
library(ggplot2)
library(dplyr)
library(readxl)
library(stats)
library(tidyr)

```


## Daten laden und vorbereiten

Um die Beziehung zwischen Rabatten und Verkaufsleistung bei Jelmoli am 27.12.23 zu untersuchen, werden die Rabattdaten in vier einfache Kategorien eingeteilt: "nicht reduziert", "niedrig", "mittel" und "hoch". Diese Einteilung hilft mir zu verstehen, ob hÃ¶here Rabatte tatsÃ¤chlich zu einem besseren Absatz fÃ¼hren. Durch die Gruppierung der Rabatte kann klarer gesehen werden, welche Rabattstufen den grÃ¶ssten Einfluss auf den Verkauf haben. HierfÃ¼r mÃ¶chte ich den Lagerbestandsabbau in % sehen. Diese Vorbereitungen bilden die Basis fÃ¼r den ğœ’Â²-Goodness-of-Fit-Test und ermÃ¶glicht es, Erkenntnisse Ã¼ber die EffektivitÃ¤t der Rabattstrategien zu gewinnen.

```{r}
# Daten laden
daten_1 <- read_excel("/Users/cristianruiz/Desktop/CAS/R/Semesterarbeit 4/Sale_Sport_27.12.2023.xlsx")

# Daten prÃ¼fen
head(daten_1)

# Datenbereinigung und Transformation mit Lagerbestandsabbau in %
daten_1 <- daten_1 %>%
  filter(!is.na(`Aktueller Rabatt`) & !is.na(Absatz) & !is.na(`Anfangsbestand Menge`)) %>%  # Zeilen mit fehlenden Werten entfernen
  mutate(
    LagerbestandsabbauProzent = (Absatz / `Anfangsbestand Menge`) * 100  # Lagerbestandsabbau in Prozent berechnen
  ) %>%
  filter(LagerbestandsabbauProzent <= 100 & LagerbestandsabbauProzent >= 0) %>%  # Nur realistische Werte behalten
  mutate(
    RabattKategorie = case_when(
      `Aktueller Rabatt` == 0 ~ "nicht reduziert",
      `Aktueller Rabatt` <= -0.7 ~ "hoch",
      `Aktueller Rabatt` <= -0.5 ~ "mittel",
      `Aktueller Rabatt` < 0 ~ "niedrig",
      TRUE ~ as.character(NA)  # FÃ¼r den Fall, dass keine der Bedingungen zutrifft
    )
  ) %>%
  drop_na(RabattKategorie)  # Zeilen entfernen, die keine Rabattkategorie haben


```

## HÃ¤ufigkeitsverteilung als Histogramm

Erstellung des Histogramms der Rabattkategorien, um die HÃ¤ufigkeitsverteilung zu visualisieren.

```{r}
ggplot(daten_1, aes(x = RabattKategorie)) +
  geom_bar(fill = "skyblue", color = "black") +
  
  labs(x = "Rabattkategorie", y = "HÃ¤ufigkeit", title = "Histogramm 1: Rabattkategorien") +
  theme_minimal()

```


Die Daten zeigen, wie vermutet, dass der Anteil der hochrabattierten Artikel vergleichsweise gering ist, wÃ¤hrend der Anteil der nichtreduzierten Artikel am hÃ¶chsten ausfÃ¤llt. Um einen detaillierten Einblick in den durchschnittlichen Lagerabbau in Prozent zu erhalten, betrachten wir die nachfolgende Grafik. Diese visualisiert den durchschnittlichen Lagerbestandsabbau, aufgeschlÃ¼sselt nach den einzelnen Rabattkategorien (Histogramm 2: Durchschnittlicher Lagerbestandsabbau nach Rabattkategorie).


```{r}

# Durchschnittlichen Lagerbestandsabbau fÃ¼r jede Rabattkategorie berechnen
durchschnittlicher_abbauprozentsatz <- daten_1 %>%
  group_by(RabattKategorie) %>%
  summarise(DurchschnittlicherAbbau = mean(LagerbestandsabbauProzent))

# Balkendiagramm des durchschnittlichen Lagerbestandsabbaus 
ggplot(durchschnittlicher_abbauprozentsatz, aes(x = RabattKategorie, y = DurchschnittlicherAbbau, fill = RabattKategorie)) +
  geom_bar(stat = "identity", color = "black") +
  labs(x = "Rabattkategorie", y = "Durchschnittlicher Lagerbestandsabbau in Prozent", title = "Histogramm 2: Durchschnittlicher Lagerbestandsabbau nach Rabattkategorie") +
  theme_minimal()

```

Es wird deutlich, dass der durchschnittliche Lagerabbau in Prozent mit zunehmendem Rabattsatz steigt.

## Theoretische vs tatsÃ¤chliche Verteilung und der ğœ’2-goodness of fit Test

ZunÃ¤chst betrachten wir die Verteilung der Lagerbestandsabbau-ProzentsÃ¤tze Ã¼ber alle Rabattkategorien hinweg. Dies wird in der nachfolgenden Grafik dargestellt (Histogramm 3: Erwartete Verteilung des Lagerbestandsabbaus).

```{r}

# ggplot fÃ¼r die tatsÃ¤chliche Verteilung
ggplot(daten_1, aes(x = LagerbestandsabbauProzent)) +
  geom_density(aes(fill = RabattKategorie), alpha = 0.5) +
  scale_x_continuous(name = "Lagerbestandsabbau in Prozent", 
                     limits = c(5, max(daten_1$LagerbestandsabbauProzent, na.rm = TRUE))) +
  scale_y_continuous(name = "Dichte") +
  labs(title = "Histogramm 3: Erwartete Verteilung des Lagerbestandsabbaus") +
  theme_minimal()

```
Die Analyse bestÃ¤tigt erneut, dass Artikel mit geringen oder keinen Rabatten tendenziell in den niedrigeren Prozentbereichen des Lagerabbaus zu finden sind. Im Gegensatz dazu zeigt die Kategorie 'hoch' eine Linksschiefe, was auf eine hÃ¶here Anzahl an VerkÃ¤ufen mit grossen Lagerabbau-ProzentsÃ¤tzen hinweist.

Bei der genaueren Betrachtung der Kategorie 'nicht reduziert' fÃ¤llt auf, dass die Verteilung des Lagerabbaus Merkmale einer Log-Normalverteilung aufweist â€“ sie Ã¤hnelt einer Normalverteilung, weist jedoch eine Rechtsschiefe auf. Um dies weiter zu untersuchen, wird nachfolgend ein Histogramm erstellt, das die tatsÃ¤chliche Verteilung mit einer theoretischen Log-Normalverteilung vergleicht.

```{r}
# Filtern der Kategorie "nicht reduziert" und Entfernen von Zeilen mit Null-, Negativ- oder ungÃ¼ltigen Werten
daten_nicht_rabattiert <- daten_1 %>%
  filter(RabattKategorie == "nicht reduziert" & 
         LagerbestandsabbauProzent > 0 & 
         !is.infinite(LagerbestandsabbauProzent) & 
         !is.na(LagerbestandsabbauProzent))

# Berechnung der Parameter fÃ¼r die Log-Normalverteilung, nachdem Null- oder Negativwerte ausgeschlossen wurden
log_daten <- log(daten_nicht_rabattiert$LagerbestandsabbauProzent)
mittelwert_log <- mean(log_daten, na.rm = TRUE)
standardabweichung_log <- sd(log_daten, na.rm = TRUE)

# Erstellen des Plots mit Achsenbegrenzung fÃ¼r eine deutlichere Ansicht
ggplot(daten_nicht_rabattiert, aes(x = LagerbestandsabbauProzent)) +
  geom_density(aes(y = ..density..), fill = "skyblue", alpha = 0.5) +
  stat_function(fun = dlnorm, args = list(meanlog = mittelwert_log, sdlog = standardabweichung_log),
                color = "red", linetype = "dashed", size = 1) +
  scale_x_continuous(name = "Lagerbestandsabbau in Prozent", 
                     limits = c(5, max(daten_nicht_rabattiert$LagerbestandsabbauProzent, na.rm = TRUE))) +
  scale_y_continuous(name = "Dichte") +
  labs(title = "Histogramm 4: TatsÃ¤chliche vs. Theoretische Log-Normalverteilung fÃ¼r 'nicht reduzierte' Artikel") +
  theme_minimal()



```

Das gezeigte Histogramm verdeutlicht, dass die theoretische Log-Normalverteilung ebenfalls eine Rechtsschiefe aufweist. Allerdings stimmt sie nicht optimal mit der beobachteten Verteilung Ã¼berein. Dies kÃ¶nnte ein Hinweis darauf sein, dass die Log-Normalverteilung mÃ¶glicherweise nicht die passendste Wahl fÃ¼r diese Daten ist.

Um diese Vermutung zu Ã¼berprÃ¼fen, wenden wir als nÃ¤chsten Schritt den ğœ’Â²-Goodness-of-Fit-Test an.

```{r}

# Beobachtete HÃ¤ufigkeiten fÃ¼r "nicht-reduziert" berechnen
bins <- seq(0, 100, by = 10)
beobachtet <- hist(daten_nicht_reduziert$LagerbestandsabbauProzent, breaks = bins, plot = FALSE)$counts

# Erwartete HÃ¤ufigkeiten basierend auf der Log-Normalverteilung berechnen
erwartet <- rep(0, length(bins)-1)
for (i in 1:(length(bins)-1)) {
  erwartet[i] <- plnorm(bins[i+1], meanlog = mittelwert_log, sdlog = standardabweichung_log) -
                 plnorm(bins[i], meanlog = mittelwert_log, sdlog = standardabweichung_log)
}
erwartet <- erwartet * sum(beobachtet) # Anpassen der erwarteten HÃ¤ufigkeiten

# ğœ’Â²-Goodness-of-Fit-Test durchfÃ¼hren
chi_quadrat_test <- chisq.test(beobachtet, p = erwartet, rescale.p = TRUE)

# Ergebnisse
chi_quadrat_test

```

Die Ergebnisse des Chi-Quadrat-Tests fÃ¼r die beobachteten Daten zeigen ein X-Quadrat von 23.451 mit 9 Freiheitsgraden und einen sehr niedrigen p-Wert von 0.005259. Dies bedeutet, dass die Log-Normalverteilung keine adÃ¤quate Ãœbereinstimmung mit den tatsÃ¤chlichen Daten aufweist. Der niedrige p-Wert fÃ¼hrt zur Ablehnung der Nullhypothese, welche besagt, dass die beobachteten Daten der angenommenen Log-Normalverteilung entsprechen. Die signifikante Diskrepanz zwischen der theoretischen und der beobachteten Verteilung weist darauf hin, dass eine andere Verteilung mÃ¶glicherweise besser geeignet ist.

In Anbetracht der Tatsache, dass die Verteilung sich im Intervall von 0% bis 100% befindet, eine Untergrenze von 0% hat und eine Spitze bei einer bestimmten Dichte zeigt, kÃ¶nnte eine Beta-Verteilung eine passendere theoretische Verteilung sein [Quelle](https://www.statistik-nachhilfe.de/ratgeber/statistik/wahrscheinlichkeitsrechnung-stochastik/wahrscheinlichkeitsverteilungen/stetige-verteilungen/beta-verteilung). Daher wird der nÃ¤chste Schritt die SchÃ¤tzung der Parameter fÃ¼r eine Beta-Verteilung sein. Die Parameter werden dabei mit der Methode der Momente geschÃ¤tzt [Quelle](https://search.r-project.org/CRAN/refmans/EnvStats/html/ebeta.html).



```{r}
# Berechnung des Mittelwerts und der Varianz der empirischen Daten
mittelwert <- mean(daten_nicht_rabattiert$LagerbestandsabbauProzent / 100)
varianz <- var(daten_nicht_rabattiert$LagerbestandsabbauProzent / 100)

# SchÃ¤tzen der Parameter der Beta-Verteilung mit der Methode der Momente
alpha_schaetzung <- ((1 - mittelwert) / varianz - 1 / mittelwert) * mittelwert^2
beta_schaetzung <- alpha_schaetzung * (1 / mittelwert - 1)

#Plotten der tatsÃ¤chlichen und der theoretischen Verteilung mit den neuen Parametern
ggplot(daten_nicht_rabattiert, aes(x = LagerbestandsabbauProzent / 100)) +
geom_density(aes(y = ..density..), fill = "skyblue", alpha = 0.5) +
stat_function(fun = dbeta, args = list(shape1 = alpha_schaetzung, shape2 = beta_schaetzung),
colour = "red", linetype = "dashed", size = 1) +
labs(title = "Histogramm 5: TatsÃ¤chliche vs. Theoretische Beta-Verteilung",
x = "Lagerbestandsabbau in Prozent", y = "Dichte") +
theme_minimal()
  



```

Wie aus Histogramm 5 ersichtlich, zeigt sich, dass die theoretische Verteilung auch keine Ãœbereinstimmung mit der tatsÃ¤chlichen Verteilung aufweist. Um diese Beobachtung zu bestÃ¤tigen, fÃ¼hren wir als nÃ¤chsten Schritt den ğœ’Â²-Goodness-of-Fit-Test durch.

```{r}

# Berechnen der beobachteten HÃ¤ufigkeiten in den Intervallen
bins <- seq(0, 1, length.out = 11) # 10 Intervalle von 0 bis 1
beobachtet <- hist(daten_nicht_rabattiert$LagerbestandsabbauProzent / 100, breaks = bins, plot = FALSE)$counts

# Berechnen der erwarteten HÃ¤ufigkeiten basierend auf der Beta-Verteilung mit den geschÃ¤tzten Parametern
erwartet <- rep(0, length(bins)-1)
for(i in 1:(length(bins)-1)) {
  erwartet[i] <- pbeta(bins[i+1], shape1 = alpha, shape2 = beta) - pbeta(bins[i], shape1 = alpha, shape2 = beta)
}
erwartet <- erwartet * sum(beobachtet) # Anpassen der erwarteten HÃ¤ufigkeiten an die Gesamtzahl der Beobachtungen

# DurchfÃ¼hren des ğœ’Â²-Goodness-of-Fit-Tests
chi_quadrat_test <- chisq.test(beobachtet, p = erwartet, rescale.p = TRUE)

# Ergebnisse ausgeben
print(chi_quadrat_test)

```
Die Ergebnisse des Chi-Quadrat-Tests zeigen weiterhin einen extrem niedrigen p-Wert von 2.2e-16. Dies fÃ¼hrt zur Ablehnung der Nullhypothese, was bedeutet, dass wir nach einer besser passenden theoretischen Verteilung suchen mÃ¼ssen. Nachdem bereits zwei theoretisch passende Verteilungen untersucht wurden, ist es nun wichtig, die Daten erneut sorgfÃ¤ltig zu Ã¼berprÃ¼fen. Dabei liegt der Fokus darauf, mÃ¶gliche Anomalien zu identifizieren, die das Ergebnis des ğœ’Â²-Goodness-of-Fit-Tests beeinflussen kÃ¶nnten.


Kontrolle der Daten:

```{r}
# Allgemeine ÃœberprÃ¼fung
summary(daten_nicht_rabattiert$LagerbestandsabbauProzent)

# PrÃ¼fung ob Summe = 0 und somit keine NA Werte vorhanden.
sum(is.na(daten_nicht_rabattiert$LagerbestandsabbauProzent))

# Begutachtung des Boxplotes
boxplot(daten_nicht_rabattiert$LagerbestandsabbauProzent, main = "Boxplot 1: LagerbestandsabbauProzent")

```
Bei genauerer Betrachtung der allgemeinen Daten und der ÃœberprÃ¼fung, ob die Summe aller Werte in der Kategorie 'nicht reduziert' Null ergibt, sind keine Anomalien erkennbar. Allerdings zeigt die Analyse des Boxplots, dass es Ausreisser im oberen Bereich gibt. Diese kÃ¶nnten mÃ¶glicherweise die Ergebnisse des ğœ’Â²-Goodness-of-Fit-Tests beeinflussen.

Um die Auswirkungen der Ausreisser zu minimieren und die Verteilungsschiefe zu verringern, werde ich als nÃ¤chsten Schritt eine Logarithmus-Transformation auf die Daten anwenden und die Anpassung an eine Log-Normalverteilung erneut Ã¼berprÃ¼fen.

```{r}

# Filtern der Kategorie "nicht reduziert" und Entfernen von Zeilen mit Null-, Negativ- oder ungÃ¼ltigen Werten
daten_nicht_rabattiert <- daten_nicht_reduziert %>%
  filter(RabattKategorie == "nicht reduziert" & 
         LagerbestandsabbauProzent > 0 & 
         !is.infinite(LagerbestandsabbauProzent) & 
         !is.na(LagerbestandsabbauProzent))

# Logarithmus-Transformation der Daten
log_daten <- log(daten_nicht_rabattiert$LagerbestandsabbauProzent)

# Berechnung der Parameter fÃ¼r die Log-Normalverteilung
mittelwert_log <- mean(log_daten, na.rm = TRUE)
standardabweichung_log <- sd(log_daten, na.rm = TRUE)

# Anzahl der Intervalle
bins <- seq(0, 100, by = 20)

# Beobachtete HÃ¤ufigkeiten fÃ¼r "nicht-reduziert" berechnen
beobachtet <- hist(daten_nicht_rabattiert$LagerbestandsabbauProzent, breaks = bins, plot = FALSE)$counts

# Erwartete HÃ¤ufigkeiten basierend auf der Log-Normalverteilung berechnen
erwartet <- rep(0, length(bins)-1)
for (i in 1:(length(bins)-1)) {
  erwartet[i] <- plnorm(bins[i+1], meanlog = mittelwert_log, sdlog = standardabweichung_log) -
                 plnorm(bins[i], meanlog = mittelwert_log, sdlog = standardabweichung_log)
}
erwartet <- erwartet * sum(beobachtet) # Anpassen der erwarteten HÃ¤ufigkeiten

# Chi-Quadrat-Goodness-of-Fit-Test durchfÃ¼hren
chi_quadrat_test <- chisq.test(beobachtet, p = erwartet, rescale.p = TRUE)

# Ergebnisse
chi_quadrat_test


```
Obwohl der P-Wert immer noch sehr niedrig ist, fÃ¤llt er im Vergleich zu den vorherigen Tests etwas hÃ¶her aus. Aufgrund dieser Beobachtung treffe ich die Entscheidung, die Ausreisser aus den Daten zu entfernen. Dies mache ich indem ich die Ausreisser aus dem oberen Bereich des Lagerabbaus in Prozent, also der Bereich von 95% - 100%, ausklammere, was soll dazu beitragen soll, den ğœ’Â²-Goodness-of-Fit-Test unter klareren Bedingungen durchzufÃ¼hren und zuverlÃ¤ssigere Ergebnisse zu erzielen.

```{r}


# Filtern der Kategorie "nicht reduziert" und Entfernen von Zeilen mit Null-, Negativ- oder ungÃ¼ltigen Werten
daten_nicht_rabattiert <- daten_nicht_reduziert %>%
  filter(RabattKategorie == "nicht reduziert" & 
         LagerbestandsabbauProzent > 0-95 & 
         !is.infinite(LagerbestandsabbauProzent) & 
         !is.na(LagerbestandsabbauProzent))

# Logarithmus-Transformation der Daten
log_daten <- log(daten_nicht_rabattiert$LagerbestandsabbauProzent)

# Berechnung der Parameter fÃ¼r die Log-Normalverteilung
mittelwert_log <- mean(log_daten, na.rm = TRUE)
standardabweichung_log <- sd(log_daten, na.rm = TRUE)

# Anzahl der Intervalle
bins <- seq(0, 100, by = 20)

# Beobachtete HÃ¤ufigkeiten fÃ¼r "nicht-reduziert" berechnen
beobachtet <- hist(daten_nicht_rabattiert$LagerbestandsabbauProzent, breaks = bins, plot = FALSE)$counts

# Erwartete HÃ¤ufigkeiten basierend auf der Log-Normalverteilung berechnen
erwartet <- rep(0, length(bins)-1)
for (i in 1:(length(bins)-1)) {
  erwartet[i] <- plnorm(bins[i+1], meanlog = mittelwert_log, sdlog = standardabweichung_log) -
                 plnorm(bins[i], meanlog = mittelwert_log, sdlog = standardabweichung_log)
}
erwartet <- erwartet * sum(beobachtet) # Anpassen der erwarteten HÃ¤ufigkeiten

# Chi-Quadrat-Goodness-of-Fit-Test durchfÃ¼hren
chi_quadrat_test <- chisq.test(beobachtet, p = erwartet, rescale.p = TRUE)

# Ergebnisse
chi_quadrat_test
```

Die Ergebnisse des Chi-Quadrat-Tests zeigen ein X-Quadrat von 5.8689 mit 4 Freiheitsgraden und einen p-Wert von 0.2092. Da dieser p-Wert Ã¼ber dem Ã¼blichen Signifikanzniveau von 0.05 liegt, gibt es keine ausreichende statistische Grundlage, um die Nullhypothese abzulehnen. Folglich kann davon ausgegangen werden, dass die beobachteten Daten mit der angenommenen theoretischen Verteilung Ã¼bereinstimmen.

## Fazit

Diese Analyse untersuchte den Zusammenhang zwischen Rabatten und Verkaufsleistung bei der Jelmoli AG, wobei der Fokus darauf lag zu bestimmen, ob nicht rabattierte Artikel im Vergleich zu rabattierten Artikeln eine signifikant schlechtere VerkaufshÃ¤ufigkeit aufweisen. Durch die Kategorisierung der Rabatte und die anschliessende Visualisierung in Histogrammen konnten interessante Einblicke in die Verteilung des Lagerbestandsabbaus gewonnen werden. Es zeigte sich, dass der durchschnittliche Lagerabbau mit zunehmendem Rabattsatz deutlich steigt.

In der weiteren Analyse wurde versucht, die beobachteten Daten mit verschiedenen theoretischen Verteilungen abzugleichen. Sowohl die Log-Normalverteilung als auch die Beta-Verteilung erwiesen sich zunÃ¤chst als nicht passend, wie die Ergebnisse der ğœ’Â²-Goodness-of-Fit-Tests zeigten. Eine detailliertere Betrachtung der Daten, insbesondere der Ausreisser, und die Anwendung einer Logarithmus-Transformation fÃ¼hrten schliesslich zu einer besseren Ãœbereinstimmung mit der Log-Normalverteilung.


# Teil 2

## Einleitung

Im Rahmen der explorativen Datenanalyse ist es oft nÃ¼tzlich, den Zusammenhang zwischen verschiedenen metrischen Variablen zu untersuchen. Ein gÃ¤ngiges Verfahren hierfÃ¼r ist die lineare Einfachregression, bei der der lineare Zusammenhang zwischen einer abhÃ¤ngigen und einer unabhÃ¤ngigen Variablen modelliert wird. Diese Analyse wird ergÃ¤nzt durch diagnostische Plots und die Bestimmung von Vertrauensintervallen, um die QualitÃ¤t und ZuverlÃ¤ssigkeit des Modells zu beurteilen.



### a) Auswahl eines Datensatzes und Darstellung einer Matrix mit Streudiagrammen und Korrelationskoeffizienten 

ZunÃ¤chst wird ein geeigneter Datensatz ausgewÃ¤hlt. In vorliegender Arbeit wurde der Datensatz `mtcars` verwendet, ein eingebauter Datensatz in R, der verschiedene Merkmale von Autos enthÃ¤lt. Aus diesem Datensatz wurden die metrischen Variablen "mpg" (Meilen pro Gallone), "hp" (PferdestÃ¤rken), "wt" (Gewicht) und "qsec" (Zeit bis 1/4 Meile) ausgewÃ¤hlt. Mit Hilfe des `GGally` Pakets und der Funktion `ggpairs` wird eine Matrix von Streudiagrammen erstellt, in der paarweise die Beziehungen zwischen diesen Variablen zusammen mit den Korrelationskoeffizienten dargestellt werden.

```{r}
install.packages("GGally")
library(GGally)
data(mtcars)
# WÃ¤hlen der metrischen Variablen
metrische_variabeln <- mtcars[, c("mpg", "hp", "wt", "qsec")]
# Erstellen einer Matrix von Streudiagrammen mit Korrelationskoeffizienten
ggpairs(metrische_variabeln)

```

#### ErklÃ¤rung der Ergebnisse

- **"mpg" und "hp"**: Es gibt eine negative Korrelation (-0.776), was darauf hindeutet, dass Autos mit hÃ¶herer Leistung (mehr PferdestÃ¤rken) dazu neigen, weniger effizient im Kraftstoffverbrauch (weniger Meilen pro Gallone) zu sein.

- **"mpg" und "wt"**: Die Korrelation ist ebenfalls negativ (-0.868), was darauf hinweist, dass schwerere Autos (hÃ¶heres Gewicht) tendenziell einen hÃ¶heren Kraftstoffverbrauch (weniger Meilen pro Gallone) aufweisen. Dies ist die stÃ¤rkste Korrelation in der Matrix und impliziert einen starken umgekehrten Zusammenhang.

- **"mpg" und "qsec"**: Die Korrelation ist positiv (0.419), was bedeutet, dass Autos, die lÃ¤nger brauchen, um eine Viertelmeile zurÃ¼ckzulegen (hÃ¶herer qsec-Wert), tendenziell effizienter im Kraftstoffverbrauch sind (mehr Meilen pro Gallone). Diese Korrelation ist jedoch schwÃ¤cher als die anderen und mit einem Sternchen markiert, was auf ein niedrigeres Signifikanzniveau hinweist.

- **"hp" und "wt"**: Die Korrelation ist positiv (0.659), was anzeigt, dass schwerere Autos dazu neigen, mehr PferdestÃ¤rken zu haben.

- **"hp" und "qsec"**: Hier besteht eine starke negative Korrelation (-0.708), die darauf hinweist, dass Autos mit mehr PferdestÃ¤rken schneller eine Viertelmeile zurÃ¼cklegen kÃ¶nnen (niedrigerer qsec-Wert).

- **"wt" und "qsec"**: Die Korrelation ist nicht signifikant (-0.175), was darauf hinweist, dass es keinen starken linearen Zusammenhang zwischen dem Gewicht eines Autos und der Zeit, die es braucht, um eine Viertelmeile zurÃ¼ckzulegen, gibt.


### b) Auswahl eines Variablenpaares und Erstellung eines Streudiagramms

Basierend auf den Erkenntnissen aus Aufgabe a wurde das Variablenpaar "mpg" (Meilen pro Gallone) und "wt" (Gewicht) fÃ¼r die weitere Analyse ausgewÃ¤hlt. Die Wahl fiel auf dieses Paar, da es die stÃ¤rkste negative Korrelation (-0.868) in der Korrelationsmatrix aufwies, was auf einen starken umgekehrten Zusammenhang zwischen dem Gewicht der Autos und ihrem Kraftstoffverbrauch hindeutet. Ein Streudiagramm dieser beiden Variablen wird erstellt, um visuell den Zusammenhang zwischen dem Gewicht des Autos und dem Kraftstoffverbrauch zu untersuchen.


```{r}
# Wahl des Paares metrischer Variablen "mpg" und "wt"
plot(mtcars$wt, mtcars$mpg, xlab = "Gewicht (wt)", ylab = "Meilen pro Gallone (mpg)", main = "Streudiagramm von Gewicht vs. MPG")

```

#### Streudiagramm von Gewicht vs. MPG

Das Streudiagramm zeigt eine deutliche negative Beziehung zwischen dem Gewicht der Autos (X-Achse) und ihrem Kraftstoffverbrauch (Y-Achse). Wie wir aus der Korrelationsmatrix in Aufgabe a) wissen, betrÃ¤gt der Korrelationskoeffizient fÃ¼r diese beiden Variablen -0.868, was auf eine starke negative Korrelation hindeutet. Dies wird im Streudiagramm visualisiert, wo man sehen kann, dass Autos mit geringerem Gewicht tendenziell mehr Meilen pro Gallone zurÃ¼cklegen, wÃ¤hrend schwerere Autos weniger effizient sind. Diese visuelle Darstellung unterstÃ¼tzt die Entscheidung, "mpg" und "wt" fÃ¼r die lineare Regressionsanalyse zu verwenden, da sie stark darauf hindeutet, dass das Gewicht ein wichtiger PrÃ¤diktor fÃ¼r den Kraftstoffverbrauch ist.

### c) DurchfÃ¼hrung der linearen Einfachregression und Plot der diagnostischen Plots

Nachdem die starke negative Korrelation zwischen "mpg" und "wt" im `mtcars` Datensatz sowohl durch die Korrelationsmatrix als auch das darauffolgende Streudiagramm bestÃ¤tigt wurde, soll nun der Einfluss von "wt" auf "mpg" durch eine lineare Einfachregression nÃ¤her quantifiziert werden. Dazu wird die `lm` Funktion in R genutzt, um ein lineares Modell zu erstellen, wobei "mpg" die abhÃ¤ngige Variable (Reaktion) und "wt" die unabhÃ¤ngige Variable darstellt. Der Modellbericht gibt Aufschluss Ã¼ber die GÃ¼te der Anpassung und die statistische Signifikanz der Regressionsergebnisse.


```{r}
# Erstellen des lineares Regressionsmodell
model <- lm(mpg ~ wt, data = mtcars)
# Modellbericht anzeigen
summary(model)

# Plotten der Residuen und QQ-Plot
par(mfrow=c(2, 2)) # Einstellen des Plot-Bereichs
plot(model)         # Erstellt automatisch 4 Diagnoseplots

```

#### ErklÃ¤rung der Plots

- **Residuals vs Fitted**: Dieser Plot zeigt keine klaren Muster oder systematischen Trends, was darauf hindeutet, dass das Modell keine systematischen Fehler aufweist. Die Residuen scheinen zufÃ¤llig um die Nulllinie verteilt zu sein, was gut ist. Es gibt jedoch einige Punkte, die weiter von der Nulllinie entfernt liegen, was auf mÃ¶gliche Ausreisser hinweist.

- **Q-Q Plot der Residuen**: Die Punkte in diesem Plot folgen weitgehend der diagonalen Linie, was darauf hindeutet, dass die Residuen annÃ¤hernd normalverteilt sind. Es gibt einige Abweichungen in den Extremen, was auf mÃ¶gliche Ausreisser oder Schwanzverhalten hinweist, das von der Normalverteilung abweicht.

- **Scale-Location **: Die gleichmÃ¤ssige Verteilung der Punkte in diesem Plot und das Fehlen eines erkennbaren Musters deuten darauf hin, dass die Varianz der Residuen Ã¼ber die Vorhersagewerte hinweg konstant ist, was fÃ¼r HomoskedastizitÃ¤t spricht. Es gibt keine klaren Anzeichen von HeteroskedastizitÃ¤t.

- **Residuen vs Hebelwerte**: Dieser Plot zeigt keine Beobachtungen mit hohen Leverage-Werten, was darauf hindeutet, dass es keine besonders einflussreichen Datenpunkte gibt, die die Regression unverhÃ¤ltnismÃ¤ssig beeinflussen. Es gibt jedoch einige Punkte ausserhalb der Cook's Distanz Linie, was darauf hindeutet, dass es einige potentiell einflussreiche Beobachtungen gibt, die weitere Untersuchungen erfordern kÃ¶nnten.

## Fazit

Mit Hilfe von Korrelationsanalysen und Streudiagrammen konnten wir feststellen, dass ein stark negativer Zusammenhang zwischen dem Gewicht der Autos (wt) und ihrem Kraftstoffverbrauch (mpg) im `mtcars` Datensatz besteht. Dieses Ergebnis war statistisch signifikant und wurde durch die visuelle Inspektion der Daten bestÃ¤tigt.

Die durchgefÃ¼hrte lineare Einfachregression ergab ein Modell, das die Beziehung zwischen "mpg" und "wt" quantifiziert. Der Modellbericht und die diagnostischen Plots haben gezeigt, dass das Modell eine angemessene Passung hat, mit Residuen, die weitgehend keine systematischen Muster aufweisen und annÃ¤hernd normalverteilt sind. Obwohl einige potenzielle Ausreisser und einflussreiche Beobachtungen identifiziert wurden, scheint das Modell insgesamt robust zu sein.

Insgesamt liefert diese Untersuchung wertvolle Erkenntnisse Ã¼ber den Einfluss des Gewichts auf den Kraftstoffverbrauch und betont die Bedeutung der ÃœberprÃ¼fung der Modellannahmen durch diagnostische Plots. Die Ergebnisse unterstreichen auch die Notwendigkeit, die ModellqualitÃ¤t kritisch zu betrachten und Ausreisser sowie einflussreiche Datenpunkte zu identifizieren, die die Ergebnisse verzerren kÃ¶nnten.

Diese Analyse verdeutlicht, dass in der explorativen Datenanalyse sowohl grafische als auch quantitative Methoden Hand in Hand gehen, um ein umfassendes VerstÃ¤ndnis der untersuchten Daten zu erlangen.

